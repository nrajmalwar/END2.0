{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END2_07_Question_Answer_Dataset.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrajmalwar/END2.0/blob/main/Session_07/Part2_Seq2Seq_Datasets/Question_Answer_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zf6W6tBMBJu"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeHHoo-LMDPH",
        "outputId": "fb5b4840-585d-443b-9a00-e7037bfa1213"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 24 14:06:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEKUrU5GMEp2"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27S03zbAAPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb7cbe2-ddff-44b2-8432-b1ece4d58985"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator,TabularDataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Use GPU as device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device: \", device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGS-Zzs2Aglb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11098b24-9ae8-43a3-b86b-86c90939aa6f"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzCG3kVMUlcg"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHKLFYK_Mgs9",
        "outputId": "b956fcd5-d5e6-47a3-ba94-802b8a692ea7"
      },
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-24 14:06:19--  http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8254496 (7.9M) [application/x-gzip]\n",
            "Saving to: ‘Question_Answer_Dataset_v1.2.tar.gz’\n",
            "\n",
            "Question_Answer_Dat 100%[===================>]   7.87M   206KB/s    in 33s     \n",
            "\n",
            "2021-06-24 14:06:52 (247 KB/s) - ‘Question_Answer_Dataset_v1.2.tar.gz’ saved [8254496/8254496]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXrcFX6MpVF",
        "outputId": "3810ae35-645a-4c72-b5a1-23a8dfccfe76"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question_Answer_Dataset_v1.2.tar.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9btkLmQBMqpe",
        "outputId": "3ba09082-986c-4850-a5a7-e0eba407f91f"
      },
      "source": [
        "# extract files\n",
        "!tar -xvzf Question_Answer_Dataset_v1.2.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question_Answer_Dataset_v1.2/\n",
            "Question_Answer_Dataset_v1.2/S08/\n",
            "Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/\n",
            "Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/LICENSE-S08,S09\n",
            "Question_Answer_Dataset_v1.2/README.v1.2\n",
            "Question_Answer_Dataset_v1.2/S09/\n",
            "Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3o.htm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ydoNZ-kOheZ",
        "outputId": "3250a44c-0406-421b-867b-d256f3110211"
      },
      "source": [
        "path1 = 'Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt'\n",
        "path2 = 'Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt'\n",
        "path3 = 'Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt'\n",
        "\n",
        "all_files = [path1, path2, path3]\n",
        "df = []\n",
        "\n",
        "# read from each path\n",
        "for filename in all_files:\n",
        "  data = pd.read_csv(filename, index_col=None, header=0, sep='\\t', encoding='ISO-8859-1')\n",
        "  print(f'Filename: {filename}')\n",
        "  print(f'Shape: {data.shape}\\n')\n",
        "  df.append(data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\n",
            "Shape: (1715, 6)\n",
            "\n",
            "Filename: Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt\n",
            "Shape: (825, 6)\n",
            "\n",
            "Filename: Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt\n",
            "Shape: (1458, 6)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEllCOth6Htc",
        "outputId": "4faf3c68-a77d-49f1-a50a-79d7f88c34c4"
      },
      "source": [
        "# Merge all data\n",
        "data_all = pd.concat(df, axis=0, ignore_index=True)\n",
        "data_all.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3998, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "1CzGj5v952Sq",
        "outputId": "9976c740-121e-42bf-e4ea-2f972afdbc2b"
      },
      "source": [
        "# Print 5 random samples\n",
        "data_all.sample(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "      <th>ArticleFile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>Calvin_Coolidge</td>\n",
              "      <td>What Retrieved on May?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>data/set3/a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2944</th>\n",
              "      <td>Drum</td>\n",
              "      <td>Is the drum a member of the percussion group?</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set2/a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3555</th>\n",
              "      <td>Nikola_Tesla</td>\n",
              "      <td>Was Nikola Tesla a vegetarian?</td>\n",
              "      <td>Yes</td>\n",
              "      <td>hard</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set4/a3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1619</th>\n",
              "      <td>Uruguay</td>\n",
              "      <td>Is uruguay's landscape mountainous?</td>\n",
              "      <td>not really?</td>\n",
              "      <td>hard</td>\n",
              "      <td>too hard</td>\n",
              "      <td>data/set2/a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3716</th>\n",
              "      <td>San_Francisco</td>\n",
              "      <td>How large is the population of San Francisco?</td>\n",
              "      <td>San Francisco has an estimated population of 8...</td>\n",
              "      <td>medium</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set3/a8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ArticleTitle  ...   ArticleFile\n",
              "268   Calvin_Coolidge  ...  data/set3/a9\n",
              "2944             Drum  ...  data/set2/a4\n",
              "3555     Nikola_Tesla  ...  data/set4/a3\n",
              "1619          Uruguay  ...  data/set2/a9\n",
              "3716    San_Francisco  ...  data/set3/a8\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlz_I55J6sYu"
      },
      "source": [
        "## Remove null or Nan  records from Question and Answer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4SpvfWN6c8U",
        "outputId": "e1bb13cb-13f7-4184-80f1-f5248c9f292f"
      },
      "source": [
        "# Identify NaN values\n",
        "data_all.isnull().sum()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ArticleTitle                  0\n",
              "Question                     37\n",
              "Answer                      576\n",
              "DifficultyFromQuestioner    955\n",
              "DifficultyFromAnswerer      580\n",
              "ArticleFile                   2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZgHlBON6r9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d502056-6901-4c6f-a50a-3189286a570b"
      },
      "source": [
        "# Drop rows where Question or Answer is missing\n",
        "data_all.dropna(subset=['Question', 'Answer'], inplace=True)\n",
        "data_all.isnull().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ArticleTitle                  0\n",
              "Question                      0\n",
              "Answer                        0\n",
              "DifficultyFromQuestioner    688\n",
              "DifficultyFromAnswerer        5\n",
              "ArticleFile                   2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1TbwliQAfuL"
      },
      "source": [
        "# save as .tsv file\n",
        "data_all.to_csv(\"Final_data.tsv\", sep='\\t')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VSQwacl_hm6"
      },
      "source": [
        "# Tokenizer function\n",
        "spacy_en = spacy.load('en_core_web_sm') \n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "aYopLyd7__5u",
        "outputId": "e112de00-e9e9-4de9-8c2e-4770c65ac58f"
      },
      "source": [
        "data_all.sample(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "      <th>ArticleFile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>Egypt</td>\n",
              "      <td>Is Egypt the most populated country in Africa?</td>\n",
              "      <td>no</td>\n",
              "      <td>easy</td>\n",
              "      <td>medium</td>\n",
              "      <td>data/set2/a6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>James_Monroe</td>\n",
              "      <td>What dwindled and eventually died out, startin...</td>\n",
              "      <td>The Federalist Party</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set3/a2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>Montreal</td>\n",
              "      <td>What is the name of the largest church in Mont...</td>\n",
              "      <td>The largest church in Montreal is named Saint ...</td>\n",
              "      <td>medium</td>\n",
              "      <td>hard</td>\n",
              "      <td>data/set3/a7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>Amedeo_Avogadro</td>\n",
              "      <td>Is Amedeo Avogadro Italian?</td>\n",
              "      <td>Yes</td>\n",
              "      <td>hard</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set4/a8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>Korean_language</td>\n",
              "      <td>How many verb paradigms are there in Korean?</td>\n",
              "      <td>There are seven verb paradigms or speech level...</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>data/set5/a6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ArticleTitle  ...   ArticleFile\n",
              "395             Egypt  ...  data/set2/a6\n",
              "865      James_Monroe  ...  data/set3/a2\n",
              "3504         Montreal  ...  data/set3/a7\n",
              "2616  Amedeo_Avogadro  ...  data/set4/a8\n",
              "3275  Korean_language  ...  data/set5/a6\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bAIqPVLUz3_"
      },
      "source": [
        "## Field Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KqLzDBs_hbm"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "# Question and Answer correspond to columns in the .tsv file\n",
        "fields = {'Question' : ('src', SRC),\n",
        "          'Answer' : ('trg', TRG)}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulz1sQhWU_jF"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlM3gMWC_hQo"
      },
      "source": [
        "qa_data  = TabularDataset(path='Final_data.tsv',\n",
        "                          format='tsv',\n",
        "                          fields=fields\n",
        "                           )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x65_-_5c_hFz",
        "outputId": "e3b516dc-342b-49e3-9005-94827ecec871"
      },
      "source": [
        "print(\"Example of the dataset:\", vars(qa_data.examples[1868]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of the dataset: {'src': ['did', 'newton', 'reject', 'the', 'church', \"'s\", 'doctrine', 'of', 'the', 'trinity', '?'], 'trg': ['newton', 'may', 'have', 'rejected', 'the', 'church', \"'s\", 'doctrine', 'of', 'the', 'trinity', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juRx8-MdVCKm"
      },
      "source": [
        "## Train Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BeMBFg1BtWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f1cdef-967c-4e06-aa86-0b62bb42e52e"
      },
      "source": [
        "# Train-test split\n",
        "train_data, test_data = qa_data.split([0.7,0.3])\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 2395\n",
            "Number of testing examples: 1027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN6IJggTCBH6",
        "outputId": "65274392-28b3-4630-a014-58f3acc10c83"
      },
      "source": [
        "print(\"Example of training set:\", vars(train_data.examples[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of training set: {'src': ['how', 'does', 'poverty', 'in', 'san', 'francisco', 'compare', 'to', 'the', 'nation', '-', 'wide', 'average', '?'], 'trg': ['san', 'francisco', \"'s\", 'poverty', 'rate', 'is', 'lower', 'than', 'the', 'national', 'average', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x1CDBCHBWfb",
        "outputId": "831f9401-f18d-4bbf-9f68-9204df1237a4"
      },
      "source": [
        "print(\"Example of testing set:\", vars(test_data.examples[0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of testing set: {'src': ['where', 'is', 'the', 'most', 'densely', 'populated', 'part', 'of', 'canada', '?'], 'trg': ['the', 'most', 'densely', 'populated', 'part', 'of', 'the', 'country', 'is', 'the', 'quebec', 'city', '-', 'windsor', 'corridor', 'along', 'the', 'great', 'lakes', 'and', 'saint', 'lawrence', 'river', 'in', 'the', 'southeast', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5sTIHbFVFIK"
      },
      "source": [
        "## Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy0xQVgdLBkm",
        "outputId": "e1685448-daad-4aae-e191-120229f0915f"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 1979\n",
            "Unique tokens in target (en) vocabulary: 1280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEOS9g1cVH4x"
      },
      "source": [
        "## Create iterator variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GNSQSCLEOB"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    sort_key = lambda x: len(x.src),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hauQ9qe4_Od"
      },
      "source": [
        "# Seq2Seq Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddiTgU8hLFj1"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKKhHPD2LHX1"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIf8EPxVLJhx"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_u-PyW250Cd"
      },
      "source": [
        "# Model instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i0_86JVLL-9"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBhX5dKuLNar",
        "outputId": "521f8c3a-4682-4bb7-90d8-7cc5dba9c379"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(1979, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1280, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=1280, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSdevJJNLPOt",
        "outputId": "455c198c-bcb3-43dd-c702-c8301f95f1a8"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 8,847,360 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCur23Oc6jmr"
      },
      "source": [
        "# Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CvhZwYLQoT"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E28YUSd26nw4"
      },
      "source": [
        "# Train and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-HReR1sLS8w"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfm7iiOmLUhv"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXrLres2LWHg"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJJ6225z6xwG"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGdzElxhLXKm",
        "outputId": "0620fb47-bd2e-485c-93dc-4914319370ee"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), 'qa1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 6s\n",
            "\tTrain Loss: 5.203 | Train PPL: 181.762\n",
            "\t Test. Loss: 3.512 |  Test. PPL:  33.522\n",
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 4.407 | Train PPL:  82.061\n",
            "\t Test. Loss: 3.494 |  Test. PPL:  32.932\n",
            "Epoch: 03 | Time: 0m 5s\n",
            "\tTrain Loss: 4.311 | Train PPL:  74.526\n",
            "\t Test. Loss: 3.479 |  Test. PPL:  32.411\n",
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 4.247 | Train PPL:  69.867\n",
            "\t Test. Loss: 3.460 |  Test. PPL:  31.826\n",
            "Epoch: 05 | Time: 0m 5s\n",
            "\tTrain Loss: 4.195 | Train PPL:  66.372\n",
            "\t Test. Loss: 3.428 |  Test. PPL:  30.814\n",
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 4.118 | Train PPL:  61.448\n",
            "\t Test. Loss: 3.459 |  Test. PPL:  31.783\n",
            "Epoch: 07 | Time: 0m 5s\n",
            "\tTrain Loss: 4.025 | Train PPL:  55.956\n",
            "\t Test. Loss: 3.301 |  Test. PPL:  27.128\n",
            "Epoch: 08 | Time: 0m 5s\n",
            "\tTrain Loss: 3.989 | Train PPL:  53.994\n",
            "\t Test. Loss: 3.303 |  Test. PPL:  27.189\n",
            "Epoch: 09 | Time: 0m 5s\n",
            "\tTrain Loss: 3.902 | Train PPL:  49.488\n",
            "\t Test. Loss: 3.339 |  Test. PPL:  28.201\n",
            "Epoch: 10 | Time: 0m 6s\n",
            "\tTrain Loss: 3.868 | Train PPL:  47.838\n",
            "\t Test. Loss: 3.260 |  Test. PPL:  26.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SGeWF-G7YbT"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH-9wcDtLYi5",
        "outputId": "2b45a921-7df6-44fc-e08b-b48678e9f55a"
      },
      "source": [
        "model.load_state_dict(torch.load('qa1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.260 | Test PPL:  26.052 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}